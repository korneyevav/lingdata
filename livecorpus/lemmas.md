Импортируем из elan нужный слой words, убираем галочку с длительности и с сс.мс в формате времени

.*\t на ничто<br>
Копируем и вставляем в mystem<br>
Выдачу mystem вставляем в файл<br>
.+{([а-яА-ЯЁё-]*)=(.*)} на \1\n\2<br>
(\n[?!/:;,-]) на \1\n<br>
.+{(.+)} на \1\n<br>
Вставляем результат в переменную mystem в коде

Импортируем из elan нужный слой words опять<br>
\t[А-Яа-яЁё/.?=!A-Za-z-]*?\n на \n<br>
words(.*) на lemma\1\nmorph\1<br>
Вставляем результат в переменную timecodesetc в коде, стираем лишний \n в конце при необходимости<br>

Код в файле lemmatise.py

Запускаем код<br>
Находим файл elanimport.txt, перекодируем его в UTF-8<br>
Импортируем в ELAN, ставим таб как разделитель, выставляем значения колонок<br>
Все<br>
Можно и проще немного, наверное, например, сделать автоматическую выдачу mystem через API, если оно есть, и регулярки сделать тоже через python, но я в этом не разбираюсь, а работает и так, плюс вручную надо делать не так много, и так несколько даже удобнее для отладки<br>
Надеюсь не большая проблема, что у меня немного другой формат выдачи файла - не сначала все леммы, потом вся морфология, а для каждого слова сначала одно, потом другое. Импортируется нормально, так что, наверное, это не проблема<br>
Еще, надеюсь, не проблема, что одно отдельное английское слово не стал отдельно закидывать в mystem, вряд ли это очень важно; хотя можно сделать и вручную при желании<br>

Есть несколько случаев омонимии ("свете" посчиталось как форма от "Света", а не "свет"; "эм" не как междометие, а как существительное -- в целом с междометиями есть проблемы, они не всегда нужным образом размечены), что показывает, что отсутствие учитывания синтаксических связей слова иногда мешает правильному определению его морфологии. Также, очевидно, не учитывался смысл, что иногда важно -- например, "там". Есть спорные случаи, например, учет "дома" как наречия, а не словоформы от "дом", однако это не задача морфологического анализатора, это больше теоретический вопрос. В целом же морфологический анализатор справился довольно неплохо, за исключением указанных случаев ошибки редки. Эти же случаи во многом обусловлены особенностями устной речи (междометия, "там", неправильная разметка незаконченных слов), однако не только -- выше написано о синтаксисе и семантике.<br>
